{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15.4.1 Identifying Statistical Test Types\n",
    "\n",
    "When presented with a new or foreign dataset, Jeremy knows it's good practice to familiarize yourself with each data column, the dimensions of the data, and the overall characteristics of the dataset. \n",
    "\n",
    "He's going to start this section of his analysis by exploring the different characteristics to look out for in a dataset, and what each characteristic means when it comes to analysis.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For the remainder of this module, we'll focus on data analytics using hypothesis testing and statistics. Although the discipline of statistics has many specializations and nearly limitless applications, there are only a few concepts required to get started. In this introduction, we'll cover some core statistical concepts such as:\n",
    "\n",
    "- mathematical data types\n",
    "- null and alternative hypothesis\n",
    "- p-values and hypothesis testing\n",
    "- t-test of the means\n",
    "- correlation and linear regression tests\n",
    "- comparing frequency distribution using chi-squared test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Due to the structured nature of statistical testing, if we can determine any two components of a statistical analysis (input variables, analytical question, or statistical test), we can infer the third. To simplify the process of inferring what component is needed, you can use a statistical test lookup table such as the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In addition, the remaining sections will provide supplementary notes on the in-depth statistical concepts. These will focus on common statistical interview questions, analytical \"gotchas,\" and other pertinent contexts to bolster your understanding of each statistical concept. \n",
    "\n",
    "If this is your first time learning statistical concepts, or if you're still brushing off the mental cobwebs, we suggest you delay studying these optional notes until you're comfortable with this module's core statistical concepts. Take your time with each section, and refer to the statistics cheat sheet for help.\n",
    "\n",
    "IMPORTANT\n",
    "Once you're more familiar with statistical analysis, start searching for more elaborate statistical lookup tables with non-normal, generalized statistical tests. As you practice characterizing datasets, asking questions, determining a hypothesis, and testing using statistics, the process will become easier."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15.4.2 Identify Different Data Types\n",
    "\n",
    "There are two major data types in statistics: categorical and numerical. Within these types are several subtypes, each with its own use cases. In this section, we'll describe these types and explain how to analyze data effectively."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Categorical Data\n",
    "\n",
    "Categorical data represents data characteristics or qualitative descriptions. Generally, categorical data is any data that is not measured, also known as qualitative data. Categorical data can be collected in the form of strings, true/false Boolean values, or even encoded numbers as categories (such as one for red, two for blue, three for green, etc.). \n",
    "\n",
    "Several statistical tests use categorical data to inform which groups to compare. Categorical data has three subtypes: dichotomous, ordinal, and nominal\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dichotomous Data\n",
    "\n",
    "Dichotomous data is collected from either one of two categories. For example, an online survey might collect member/non-member or demographic information. Dichotomous data can be collected in the form of true/false Boolean values, 0 or 1 binary values, or two strings.\n",
    "\n",
    "Later in the module, we'll use dichotomous data to help perform many of our comparative statistical tests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ordinal Data\n",
    "\n",
    "Ordinal data has a ranked order. Although ordinal data has a sequence, we don't necessarily know the value between each ordinal data point. Data that is collected on a value scale (e.g., movie rankings, survey results, and the Likert scale) are common forms of ordinal data. \n",
    "\n",
    "Ordinal data combines the qualitative properties of labels to the quantitative properties of scale to allow for comparative analyses. Ordinal data is very popular with research and survey groups because it allows for quantitative analysis without the need of machinery and tools to obtain measurements."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Nominal Data\n",
    "\n",
    "Nominal data is data used as labels or names for other measures. Nominal data can be as individual as an identification number or can be as general as a list of three options. \n",
    "\n",
    "Unlike ordinal data, nominal data has no ranking. Therefore, nominal data is often used with a more quantitative data type to perform an analysis. Often nominal data will be transformed using a grouping function to decrease the complexity of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Numerical Data\n",
    "\n",
    "Typically, numerical data is obtained by taking a measurement from an instrument (such as a ruler, measuring scale, sensor, etc.) or by counting. \n",
    "\n",
    "In statistics, numerical data is used to perform quantitative analysis that can produce the probability of an outcome or quantify the relationship between categories. Within numerical data there are two primary data types to consider: continuous and interval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Continuous Data\n",
    "\n",
    "Continuous data can be subdivided infinitely. For example, if you want to describe the thickness of window glass, you could measure it in x number of centimeters, millimeters, nanometers, picometers, and so on.\n",
    "\n",
    "Continuous data is typically recorded with decimal places to match the precision of the measurement. Almost all statistical tests and models use continuous data to generate precise results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Interval Data\n",
    "\n",
    "Interval data is spaced out evenly on a scale. Also known as integer data, interval data does not use decimal places and can't be subdivided. Interval data also can't be multiplied or divided.\n",
    "\n",
    "Because interval data is spaced out evenly, it can be grouped together or bucketed easily. For example, a set of integers 15, 4, 18, 10, 3 , and 5 could be collected as a group that is less than 20. \n",
    "\n",
    "Due to this property, interval data can be treated as a numerical data type or transformed into a nominal data type.\n",
    "\n",
    "Additionally, interval data can be generated through rounding continuous data at the cost of losing precision of the measurement. Therefore, interval data can be used by most statistical models as either a quantitative or qualitative variable, depending on the use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Getting Oriented with Data\n",
    "\n",
    "The easiest means of orienting ourselves on a new dataset in R is to use the head()function, which shows us the first few rows of our data frame. At any point when looking at the first few rows, we can use bracket notation (or the $ operator) to select an individual column to explore.\n",
    "\n",
    "Alternatively, if we're using RStudio, we can explore any data frame by clicking on it in our environment pane. By navigating through each column and classifying each data type, we can determine which columns provide measurement results, and which columns provide characteristics about our subjects.\n",
    "\n",
    "If we're fortunate to have context provided for a given dataset via documentation or from the data collector, we should be able to identify columns and metrics of interest. However, we have not finished characterizing our data just yet—we still need to understand how values in our data are distributed."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15.4.3 Dive Into Distributions\n",
    "\n",
    "\n",
    "When it comes to data analysis, characterizing the distribution of numerical data is as important as characterizing the different data types. If we make incorrect assumptions about the distribution of our data, our statistical results could be meaningless. \n",
    "\n",
    "In general, most basic statistical tests assume that each numerical metric follows an approximate normal distribution.\n",
    "\n",
    "In other words, we must confirm our data is normal before we can use a statistical test. If the data does not follow an approximate normal distribution, we would need to implement a more generalized (and oftentimes more complicated), non-normal statistical function.\n",
    "\n",
    "Fortunately, there are qualitative and quantitative tests we can use to test our data for normality to avoid using these more generalized functions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What Is Normal Distribution?\n",
    "\n",
    "Normal distribution, or normality, is commonly referred to as \"the bell curve,\" and describes a dataset where values farther from its mean occur less frequently than values closer to its mean.\n",
    "\n",
    "When numerical data is considered to be normally distributed, the probability of any data point follows the 68-95-99.7 rule, stating that 68.27%, 95.45%, and 99.73% (effectively 100%) of the values lie within one, two, and three standard deviations of the mean, respectively.\n",
    "\n",
    "In statistics, the central limit theorem is a key concept that states if you take sufficiently large samples of data from a dataset with mean μ (mu) and standard deviation σ (sigma), then the distribution will approximate normal distribution. Therefore, if we are using relatively large sample sizes, we should expect data to become more normally distributed.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15.4.4 Test for Normality\n",
    "\n",
    "You can test for normality during data analysis by performing a qualitative test or a quantitative test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Qualitative Test for Normality\n",
    "\n",
    "The qualitative test for normality is a visual assessment of the distribution of data, which looks for the characteristic bell curve shape across the distribution. In R, we would use ggplot2 to plot the distribution using the geom_density() function.\n",
    "\n",
    "For example, if we want to test the distribution of vehicle weights from the built-in mtcars dataset, our R code would be as follows:\n",
    "\n",
    "> ggplot(mtcars,aes(x=wt)) + geom_density() #visualize distribution using density plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The geom_density() function plots a numerical vector by creating buckets of similar values and calculating the density (number of bucket data points/total number of data points) for each bucket.\n",
    "\n",
    "The results of each bucket density calculation are plotted, connected, and smoothed out to create our distribution plot. Although our data distribution does not perfectly match the normal bell curve shape, the distribution does approximate a normal distribution and could be used for further analysis.\n",
    "\n",
    "But what if our data distribution is noisy—meaning that the dataset contains uncharacteristically large or small values at high frequency—or we need to make more informed, quantitative decisions? In these cases, we would want to perform our quantitative test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Quantitative Test for Normality\n",
    "\n",
    "The quantitative test for normality uses a statistical test to quantify the probability of whether or not the test data came from a normally distributed dataset.\n",
    "\n",
    "In most cases, data scientists will use the Shapiro-Wilk test for normality, though there are many other statistical tests available. In R, we can use the built-in stats library to perform our quantitative test with the shapiro.test() function.\n",
    "\n",
    "Type the following code into the R console to look at the shapiro.test() documentation in the Help pane:\n",
    "\n",
    ">?shapiro.test()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The shapiro.test() function only requires the numeric vector of values you wish to test. Therefore, if we want to perform a quantitative Shapiro-Wilk test on our previous example, our R code would look as follows:\n",
    "\n",
    "> shapiro.test(mtcars$wt)\n",
    "\n",
    "data:  mtcars$wt\n",
    "W = 0.94326, p-value = 0.09265\n",
    "\n",
    "Later we'll discuss what a p-value is and how it is used in statistics. For our purposes, you just need to know that if the p-value is greater than 0.05, the data is considered normally distributed.\n",
    "\n",
    "Remember that most basic statistical tests assume an approximate normal distribution. Therefore, if our p-value is around 0.05 or more, we would say that our input data meets this assumption. But what happens if our data distribution does not look like a bell curve, or the p-value of the Shapiro-Wilk tests is too small?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15.4.5 Understand Skew\n",
    "\n",
    "When dealing with relatively smaller sample sizes, our data distributions are often asymmetrical. Compared to the normal distribution, where each tail of the distribution (on either side of the mean μ) mirrors one another, the asymmetrical distribution has one distribution tail that is longer than the other. This asymmetrical distribution is commonly referred to as a skewed distribution and there are two types—left skew and right skew."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Left Skew\n",
    "\n",
    "A data distribution is considered to be left skewed, or negative skewed, if the left tail is longer than the right.\n",
    "\n",
    "When data is skewed left, from the center of the distribution curve, there is a higher probability that extreme negative values exist within our dataset. When this occurs, the mean may no longer accurately reflect the central tendency of the data. Instead, we would use the median to describe the central tendency of the data. This skew is called negative skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Right Skew\n",
    "\n",
    "A data distribution is considered to be right skewed, or positive skewed, if the right tail is longer than the left.\n",
    "\n",
    "When data is skewed right, from the center of the distribution curve, there is a higher probability that extreme positive values exist within our dataset. Once again, if this occurs, we would use the median to describe the central tendency of the data. This skew is called positive skewed."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Manage Skewness\n",
    "As with most problems in data analytics, we must approach skewness on a case-by-case basis. Depending on the severity of the skewness and the size of the dataset, there are multiple means of dealing (or not dealing) with skewness.\n",
    "\n",
    "If our dataset is large, or the skewness is very subtle, we would simply point out that our data distribution shows signs of skew during reporting or presentation.\n",
    "\n",
    "In these cases, our mean and median will be roughly the same value, and there should be minimal impact to any downstream analysis.\n",
    "\n",
    "If our dataset is smaller, or the skewness does impact the overall shape of our distribution, more action is needed. There are a few different things we can try:\n",
    "\n",
    "If possible, add more data points to our dataset to alleviate the effect of skew. However, this might not be possible or might not improve the distribution.\n",
    "\n",
    "Resample or regenerate data if we think that the data might not be representative of the original conditions or dataset.\n",
    "\n",
    "Transform our data values by normalization, using another numerical variable, or by transforming the data using an operator. The concept of transforming skewed data is very popular with scientists who deal with datasets where values can differ by orders of magnitude. \n",
    "\n",
    "One of the easiest means of transforming data is using a log-transform, where each value in the numeric dataset is transformed taking either natural log, or log10. By using a log-transformation, the effects of extreme values are reduced, and this transformation can help make each distribution tail more symmetrical.\n",
    "\n",
    "IMPORTANT\n",
    "No matter what approach is used to help reduce the skewness of a dataset, it's good practice to disclose this information in a report or to use annotations on your results. This will help the reader understand the context surrounding any results, and it will make your analysis more credible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
