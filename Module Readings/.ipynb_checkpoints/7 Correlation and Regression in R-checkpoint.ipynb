{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15.7.1 The Correlation Conundrum\n",
    "\n",
    "Another big component to his new job will be to identify patterns in data and generate predictive models. Jeremy has a little experience in generating trendlines in plots, but he has no way to quantify how well these trend lines will perform when it comes time for decision making. Jeremy realizes that he must go back and learn more statistical tests that will help him quantify the patterns and models in his data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In data analytics, we'll often ask the question \"is there any relationship between variable A and variable B?\" This concept is known in statistics as correlation. \n",
    "\n",
    "Correlation analysis is a statistical technique that identifies how strongly (or weakly) two variables are related.\n",
    "\n",
    "Correlation is quantified by calculating a correlation coefficient, and the most common correlation coefficient is the Pearson correlation coefficient.\n",
    "\n",
    "The Pearson correlation coefficient is denoted as \"r\" in mathematics and is used to quantify a linear relationship between two numeric variables. The Pearson correlation coefficient ranges between -1 and 1, depending on the direction of the linear relationship.\n",
    "\n",
    "The following image is an example of an ideal positive correlation where r = 1. \n",
    "\n",
    "a line could go at a 45 degree angle through the data points.\n",
    "\n",
    "When two variables are positively correlated, they move in the same direction. In other words, when the variable on the x-axis increases, the variable on the y-axis increases as well:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following image is an example of an ideal negative correlation where r = -1.\n",
    "\n",
    "a line reverse than the previous example through data points\n",
    "\n",
    "When two variables are negatively correlated, they move in opposite directions. In other words, when the variable on the x-axis increases, the variable on the y-axis decreases."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following image is an example of two variables with no correlation where r ≈ 0. When two variables are not correlated, their values are completely independent between one another.\n",
    "\n",
    "scattered data points"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For real-world data, it can be very difficult to determine if two variables are correlated, so we must use the Pearson correlation coefficient to calculate the correlation strength. Refer to the table below.\n",
    "\n",
    "Absolute Value of r\t                    Strength of Correlation\n",
    "r < 0.3\t                                None or very weak\n",
    "0.3 ≤ r < 0.5\t                        Weak\n",
    "0.5 ≤ r < 0.7\t                        Moderate\n",
    "r ≥ 0.7\t                                Strong"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In R, we can use our geom_point() plotting function combined with the cor() function to quantify the correlation between variables. Type the following code into the R console to look at the cor() documentation in the Help pane:\n",
    "\n",
    ">?cor()\n",
    "\n",
    "To use the cor() function to perform a correlation analysis between two numeric variables, we need to provide the following arguments:\n",
    "\n",
    "x is the first variable, which would be plotted on the x-axis.\n",
    "y is the second variable, which would be plotted on the y-axis.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As long as we are using two numeric variables, there are no other assumptions regarding our input data. To practice calculating the Pearson correlation coefficient, we'll use the mtcars dataset. Type the following in the R console:\n",
    "\n",
    "> head(mtcars)\n",
    "\n",
    "                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\n",
    "Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n",
    "Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n",
    "Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n",
    "Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n",
    "Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n",
    "Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the mtcars dataset, there are a number of numeric columns that we can use to test for correlation such as mpg, disp, hp, drat, wt, and qsec. For our example, we'll test whether or not horsepower (hp) is correlated with quarter-mile race time (qsec).\n",
    "\n",
    "First, let's plot our two variables using the geom_point() function as follows:\n",
    "\n",
    "> plt <- ggplot(mtcars,aes(x=hp,y=qsec)) #import dataset into ggplot2\n",
    "> plt + geom_point() #create scatter plot\n",
    "\n",
    "Looking at our plot, it appears that the quarter-mile time is negatively correlated with horsepower. In other words, as vehicle horsepower increases, vehicle quarter-mile time decreases."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we'll use our cor() function to quantify the strength of the correlation between our two variables:\n",
    "\n",
    "> cor(mtcars$hp,mtcars$qsec) #calculate correlation coefficient\n",
    "\n",
    "-0.7082234"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From our correlation analysis, we have determined that the r-value between horsepower and quarter-mile time is -0.71, which is a strong negative correlation.\n",
    "\n",
    "For another example, let's reuse our used_cars dataset:\n",
    "\n",
    "> used_cars <- read.csv('used_car_data.csv',stringsAsFactors = F) #read in dataset\n",
    "> head(used_cars)\n",
    "\n",
    "       Car_Name Year Selling_Price Present_Price Miles_Driven Fuel_Type Seller_Type Transmission Owner\n",
    "1          ritz 2014          3350          5590        27000    Petrol      Dealer       Manual     0\n",
    "2           sx4 2013          4750          9540        43000    Diesel      Dealer       Manual     0\n",
    "3          ciaz 2017          7250          9850         6900    Petrol      Dealer       Manual     0\n",
    "4       wagon r 2011          2850          4150         5200    Petrol      Dealer       Manual     0\n",
    "5         swift 2014          4600          6870        42450    Diesel      Dealer       Manual     0\n",
    "6 vitara brezza 2018          9250          9830         2071    Diesel      Dealer       Manual     0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For this example, we'll test whether or not vehicle miles driven and selling price are correlated. Once again, we'll plot our two variables using the geom_point() function:\n",
    "\n",
    "> plt <- ggplot(used_cars,aes(x=Miles_Driven,y=Selling_Price)) #import dataset into ggplot2\n",
    "> plt + geom_point() #create a scatter plot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Compared to our previous example, our scatter plot did not help us determine whether or not our two variables are correlated. However, let's see what happens if we calculate the Pearson correlation coefficient using the cor() function:\n",
    "\n",
    "> cor(used_cars$Miles_Driven,used_cars$Selling_Price) #calculate correlation coefficient\n",
    "\n",
    "[1] 0.02918709\n",
    "\n",
    "Our calculated r-value is 0.02, which means that there is a negligible correlation between miles driven and selling price in this dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In most cases, we'll use correlation analysis as a means of exploring data and looking for trends. Although we can calculate the correlation of each pair of numerical variables in a dataset, this process can be highly time-consuming.\n",
    "\n",
    "Instead of computing each pairwise correlation, we can use the cor() function to produce a correlation matrix. \n",
    "\n",
    "A correlation matrix is a lookup table where the variable names of a data frame are stored as rows and columns, and the intersection of each variable is the corresponding Pearson correlation coefficient. \n",
    "\n",
    "We can use the cor() function to produce a correlation matrix by providing a matrix of numeric vectors."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For example, if we want to produce a correlation matrix for our used_cars dataset, we would first need to select our numeric columns from our data frame and convert to a matrix. Then we can provide our numeric matrix to the cor() function as follows:\n",
    "\n",
    "> used_matrix <- as.matrix(used_cars[,c(\"Selling_Price\",\"Present_Price\",\"Miles_Driven\")]) #convert data frame into numeric matrix\n",
    "> cor(used_matrix)\n",
    "\n",
    "               Selling_Price Present_Price Miles_Driven\n",
    "Selling_Price    1.00000000     0.8789825   0.02918709\n",
    "Present_Price    0.87898255     1.0000000   0.20364703\n",
    "Miles_Driven     0.02918709     0.2036470   1.00000000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If we look at the correlation matrix using either rows or columns, we can identify pairs of variables with strong correlation (such as selling price versus present price), or no correlation (like our previous example of miles driven versus selling price).\n",
    "\n",
    "The correlation matrix is a very powerful data exploration tool that allows an analyst to scan large numerical datasets for variables of interest. Once the variables of interest have been identified, the analyst can move on to more rigorous data analysis and hypothesis testing."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15.7.2 Return to Linear Regression\n",
    "\n",
    "Jeremy is really starting to hit his stride. So, when his team wants to figure out how to predict one variable given another, he's pretty excited that he knows the answer: use linear regression!\n",
    "\n",
    "Of course, before he actually uses the technique, he wants to dig into just exactly how it works. After all, he's leading a team into new territory, and it's up to him to make sure they are headed in the right direction."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Earlier we learned about linear regression and how it can be used to determine our dependent y variable from an independent x variable. In a more formal definition, linear regression is a statistical model that is used to predict a continuous dependent variable based on one or more independent variables fitted to the equation of a line.\n",
    "\n",
    "In school, most students learned that the equation of a line is written as y = mx + b:\n",
    "\n",
    "y = Dependent Variable\n",
    "m = Slope\n",
    "x = Independent Variable\n",
    "b = y-intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The job of a linear regression analysis is to calculate the slope and y intercept values (also known as coefficients) that minimize the overall distance between each data point from the linear model. There are two basic types of linear regression:\n",
    "\n",
    "- Simple linear regression builds a linear regression model with one independent variable.\n",
    "\n",
    "- Multiple linear regression builds a linear regression model with two or more independent variables."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Linear regression is popular in data science because it has multiple applications. First and foremost, linear regression can be used as a predictive modeling tool where future observations and measurements can be predicted and extrapolated from a linear model. Linear regression can also be used as an exploratory tool to quantify and measure the variability of two correlated variables."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A good linear regression model should approximate most data points accurately if two variables are strongly correlated. In other words, linear regression can be used as an extension of correlation analysis. In contrast to correlation analysis, which asks whether a relationship exists between variables A and B, linear regression asks if we can predict values for variable A using a linear model and values from variable B.\n",
    "\n",
    "To answer this question, linear regression tests the following hypotheses:\n",
    "\n",
    "H0 : The slope of the linear model is zero, or m = 0\n",
    "\n",
    "Ha : The slope of the linear model is not zero, or m ≠ 0\n",
    "\n",
    "If there is no significant linear relationship, each dependent value would be determined by random chance and error. Therefore, our linear model would be a flat line with a slope of 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To quantify how well our linear model can be used to predict future observations, our linear regression functions will calculate an r-squared value.\n",
    "\n",
    "The r-squared (r2) value is also known as the coefficient of determination and represents how well the regression model approximates real-world data points. In most cases, the r-squared value will range between 0 and 1 and can be used as a probability metric to determine the likelihood that future data points will fit the linear model.\n",
    "\n",
    "When using a simple linear regression model, the r-squared metric can be approximated by calculating the square of the Pearson correlation coefficient between the two variables of interest.\n",
    "\n",
    "By combining the p-value of our hypothesis test with the r-squared value, the linear regression model becomes a powerful statistics tool that both quantifies a relationship between variables and provides a meaningful model to be used in any decision-making process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Although the interpretation of a simple linear regression is different from a multiple linear regression, their model implementation is the same. In R, we'll build our linear models using the built-in lm()function. Type the following code into the R console to look at the lm() documentation in the Help pane:\n",
    "\n",
    ">?lm()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Even though there are many optional arguments for the lm()function, the lm() function only requires us to provide two arguments:\n",
    "\n",
    "- formula: is the same R statement that we use for the aov() function. The formula statement tells R how to interpret the different variables and factors. With simple linear regression, we'll use the formula Y ~ A where Y is the column name of the dependent variable, and A is the column name of the independent variable.\n",
    "\n",
    "- data: is the name of our input data frame. The data frame should contain columns for each variable.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Similar to our t-test analysis, there are a few assumptions about our input data that must be met before we perform our statistical analysis:\n",
    "\n",
    "1. The input data is numerical and continuous.\n",
    "\n",
    "2. The input data should follow a linear pattern.\n",
    "\n",
    "3. There is variability in the independent x variable. This means that there must be more than one observation in the x-axis and they must be different values.\n",
    "\n",
    "4. The residual error (the distance from each data point to the line) should be normally distributed.\n",
    "\n",
    "Validating the fourth assumption is outside the scope of this course as it involves more robust statistical methods. However, in most real-world cases, we can expect our data to meet the fourth assumption."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Once we have our data in a single data frame that meets the assumptions of our linear regression analysis, we're ready to implement the lm() function.\n",
    "\n",
    "For practice, let's revisit our correlation example using the mtcars dataset. Using our simple linear regression model, we'll test whether or not quarter-mile race time (qsec) can be predicted using a linear model and horsepower (hp).\n",
    "\n",
    "Remember from our correlation example that our Pearson correlation coefficient's r-value was -0.71, which means there is a strong negative correlation between our variables. Therefore, we anticipate that the linear model will perform well.\n",
    "\n",
    "To create a linear regression model, our R statement would be as follows:\n",
    "\n",
    "> lm(qsec ~ hp,mtcars) #create linear model\n",
    "\n",
    "Call:\n",
    "lm(formula = qsec ~ hp, data = mtcars)\n",
    "\n",
    "Coefficients:\n",
    "(Intercept)           hp  \n",
    "   20.55635     -0.01846  \n",
    "\n",
    "The output of the lm() function will be the metrics from our model. Specifically, the lm() function returns our y intercept (Intercept) and slope (hp) coefficients. Therefore, the linear regression model for our dataset would be qsec = -0.02hp + 20.56.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To determine our p-value and our r-squared value for a simple linear regression model, we'll use the summary() function:\n",
    "\n",
    "> summary(lm(qsec~hp,mtcars)) #summarize linear model\n",
    "\n",
    "Call:\n",
    "lm(formula = qsec ~ hp, data = mtcars)\n",
    "\n",
    "Residuals:\n",
    "    Min      1Q  Median      3Q     Max \n",
    "-2.1766 -0.6975  0.0348  0.6520  4.0972 \n",
    "\n",
    "Coefficients:\n",
    "             Estimate Std. Error t value Pr(>|t|)    \n",
    "(Intercept) 20.556354   0.542424  37.897  < 2e-16 ***\n",
    "hp          -0.018458   0.003359  -5.495 5.77e-06 ***\n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 1.282 on 30 degrees of freedom\n",
    "Multiple R-squared:  0.5016,\tAdjusted R-squared:  0.485 \n",
    "F-statistic: 30.19 on 1 and 30 DF,  p-value: 5.766e-06\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Although there are a number of quantitative metrics produced by the summary(lm()) function, we are only concerned with the r-squared and p-value metrics at the bottom of the output.\n",
    "\n",
    "From our linear regression model, the r-squared value is 0.50, which means that roughly 50% of all quarter mile time predictions will be correct when using this linear model. \n",
    "\n",
    "Compared to the Pearson correlation coefficient between quarter-mile race time and horsepower of -0.71, we can confirm our r-squared value is approximately the square of our r-value.\n",
    "\n",
    "In addition, the p-value of our linear regression analysis is 5.77 x 10-6, which is much smaller than our assumed significance level of 0.05%. Therefore, we can state that there is sufficient evidence to reject our null hypothesis, which means that the slope of our linear model is not zero."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Once we have calculated our linear regression model, we can visualize the fitted line against our dataset using ggplot2.\n",
    "\n",
    "First, we need to calculate the data points to use for our line plot using our lm(qsec ~ hp,mtcars) coefficients as follows:\n",
    "\n",
    "> model <- lm(qsec ~ hp,mtcars) #create linear model\n",
    "> yvals <- model$coefficients['hp']*mtcars$hp +\n",
    "model$coefficients['(Intercept)'] #determine y-axis values from linear model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Once we have calculated our line plot data points, we can plot the linear model over our scatter plot:\n",
    "\n",
    "> plt <- ggplot(mtcars,aes(x=hp,y=qsec)) #import dataset into ggplot2\n",
    "> plt + geom_point() + geom_line(aes(y=yvals), color = \"red\") #plot scatter and linear model\n",
    "\n",
    "Using our visualization in combination with our calculated p-value and r-squared value, we have determined that there is a significant relationship between horsepower and quarter-mile time."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Although the relationship between both variables is statistically significant, this linear model is not ideal. According to the calculated r-squared value, using only quarter-mile time to predict horsepower is roughly as accurate as guessing using a coin toss. In other words, the variability we observed within our horsepower data must come from multiple sources of variance. To accurately predict future horsepower observations, we need to use a more robust model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15.7.3 Perform Multiple Linear Regression\n",
    "\n",
    "After reminding himself of how helpful linear regression is, Jeremy decides to keep going—he's really on a roll, and multiple linear regression is ready for him!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Multiple linear regression is a statistical model that extends the scope and flexibility of a simple linear regression model. Instead of using a single independent variable to account for all variability observed in the dependent variable, a multiple linear regression uses multiple independent variables to account for parts of the total variance observed in the dependent variable.\n",
    "\n",
    "As a result, the linear regression equation is no longer y = mx + b. Instead, the multiple linear regression equation becomes y = m1x1 + m2x2 + … + mnxn + b, for all independent x variables and their m coefficients.\n",
    "\n",
    "In actuality, a multiple linear regression is a simple linear regression in disguise—all of the assumptions, hypotheses, and outputs are the same. The only difference between multiple linear regression and simple linear regression is how we will evaluate the outputs.\n",
    "\n",
    "When it comes to multiple linear regression, we'll look at each independent variable to determine if there is a significant relationship with the dependent variable. Once we have evaluated each independent variable, we'll evaluate the r-squared value of the model to determine if the model sufficiently predicts our dependent variable."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To practice multiple linear regression, let's revisit our mtcars dataset. From our last example, we determined that quarter-mile time was not adequately predicted from just horsepower. To better predict the quarter-mile time (qsec) dependent variable, we can add other variables of interest such as fuel efficiency (mpg), engine size (disp), rear axle ratio (drat), vehicle weight (wt), and horsepower (hp) as independent variables to our multiple linear regression model.\n",
    "\n",
    "In R, our multiple linear regression statement is as follows:\n",
    "\n",
    "> lm(qsec ~ mpg + disp + drat + wt + hp,data=mtcars) #generate multiple linear regression model\n",
    "\n",
    "Call:\n",
    "lm(formula = qsec ~ mpg + disp + drat + wt + hp, data = mtcars)\n",
    "\n",
    "Coefficients:\n",
    "(Intercept)          mpg         disp         drat           wt           hp  \n",
    "  16.541651     0.108579    -0.008076    -0.578953     1.792793    -0.018383  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Similar to the simple linear regression, the output of multiple linear regression using the lm() function produces the coefficients for each variable in the linear equation.\n",
    "\n",
    "\n",
    "Look at the coefficients from the previous multiple linear regression statement. Then fill in the missing coefficient values for the multiple linear regression model. (Round to the nearest hundredth.)\n",
    "\n",
    "qsec = 0.11 mpg + -0.01 disp + -0.58 drat + 1.79 wt + 0.02 hp + 16.54.\n",
    "\n",
    "Because multiple linear regression models use multiple variables and dimensions, they are almost impossible to plot and visualize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have our multiple linear regression model, we need to obtain our statistical metrics using the summary()function. In your R console, use the following statement:\n",
    "\n",
    ">summary(lm(qsec ~ mpg + disp + drat + wt + hp,data=mtcars)) #generate summary statistics\n",
    "\n",
    "Call:\n",
    "lm(formula = qsec ~ mpg + disp + drat + wt + hp, data = mtcars)\n",
    "\n",
    "Residuals:\n",
    "    Min      1Q  Median      3Q     Max \n",
    "-1.6628 -0.6138  0.0706  0.4087  3.3885 \n",
    "\n",
    "Coefficients:\n",
    "             Estimate Std. Error t value Pr(>|t|)    \n",
    "(Intercept) 16.541651   3.413109   4.847 5.04e-05 ***\n",
    "mpg          0.108579   0.077911   1.394  0.17523    \n",
    "disp        -0.008076   0.004384  -1.842  0.07689 .  \n",
    "drat        -0.578953   0.551771  -1.049  0.30371    \n",
    "wt           1.792793   0.513897   3.489  0.00175 ** \n",
    "hp          -0.018383   0.005421  -3.391  0.00223 ** \n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 1.053 on 26 degrees of freedom\n",
    "Multiple R-squared:  0.7085,\tAdjusted R-squared:  0.6524 \n",
    "F-statistic: 12.64 on 5 and 26 DF,  p-value: 2.767e-06"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In addition to overall model fit and the statistical test for slope, most data scientists would be curious about the contribution of each variable to the multiple linear regression model. \n",
    "\n",
    "To determine which variables provide a significant contribution to the linear model, we must look at the individual variable p-values.\n",
    "\n",
    "In the summary output, each Pr(>|t|) value represents the probability that each coefficient contributes a random amount of variance to the linear model. \n",
    "\n",
    "According to our results, vehicle weight and horsepower (as well as intercept) are statistically unlikely to provide random amounts of variance to the linear model. In other words the vehicle weight and horsepower have a significant impact on quarter-mile race time.\n",
    "\n",
    "When an intercept is statistically significant, it means there are other variables and factors that contribute to the variation in quarter-mile time that have not been included in our model. These variables may or may not be within our dataset and may still need to be collected or observed.\n",
    "\n",
    "Despite the number of significant variables, the multiple linear regression model outperformed the simple linear regression. According to the summary output, the r-squared value has increased from 0.50 in the simple linear regression model to 0.71 in our multiple linear regression model while the p-value remained significant."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Although the multiple linear regression model is far better at predicting our current dataset, the lack of significant variables is evidence of overfitting.\n",
    "\n",
    "Overfitting means that the performance of a model performs well with a current dataset, but fails to generalize and predict future data correctly. Later in this course we'll learn more about overfitting and ways to avoid it.\n",
    "\n",
    "Depending on the dataset, the questions being asked, and the audience, a simple linear regression model may be more appropriate than a multiple linear regression model. \n",
    "\n",
    "However, the amount of information that can be obtained and analyzed will be far greater using a multiple linear regression.\n",
    "\n",
    "As with any data model, it takes practice to learn how to identify variables of interest, select an appropriate model, and refine a model to increase performance. \n",
    "\n",
    "Before moving to the next section, take some time to perform correlation analysis on our previous datasets. Then use the correlation analysis to identify potential variables of interest. Once you have variables of interest, practice generating simple and multiple linear regression models to try and create accurate predictive models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
